#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) [2025] [liuyngchng@hotmail.com] - All rights reserved.
import json
import os
import time
from typing import List, Dict

import requests

from common.vdb_meta_util import VdbMeta
from common.cfg_util import get_usr_prompt_template
from common.cm_utils import estimate_tokens
from common.docx_md_util import get_md_file_content, convert_md_to_docx, save_content_to_md_file, get_md_file_catalogue, \
    convert_docx_to_md, extract_sections_content, split_md_file_with_catalogue, split_md_content_with_catalogue
import logging.config

from common.docx_meta_util import update_process_info, get_doc_info
from common.my_enums import FileType
from common.statistic_util import add_input_token_by_uid, add_output_token_by_uid
from common.sys_init import init_yml_cfg
from common.vdb_util import search_txt
from common.xlsx_util import convert_md_to_xlsx, convert_xlsx_to_md
from common.const import MAX_SECTION_LENGTH, VDB_PREFIX

log_config_path = 'logging.conf'
if os.path.exists(log_config_path):
    logging.config.fileConfig(log_config_path, encoding="utf-8")
else:
    from common.const import LOG_FORMATTER
    logging.basicConfig(level=logging.INFO,format= LOG_FORMATTER, force=True)
logger = logging.getLogger(__name__)

# å…³é—­requestè¯·æ±‚äº§ç”Ÿçš„è­¦å‘Šä¿¡æ¯ (å®¢æˆ·ç«¯æ²¡æœ‰è¿›è¡ŒæœåŠ¡ç«¯ SSL çš„è¯ä¹¦éªŒè¯ï¼Œæ­£å¸¸ä¼šäº§ç”Ÿè­¦å‘Šä¿¡æ¯)ï¼Œ åªè¦æ¸…æ¥šè‡ªå·±åœ¨åšä»€ä¹ˆ
import urllib3
from urllib3.exceptions import InsecureRequestWarning
urllib3.disable_warnings(category=InsecureRequestWarning)

class PaperReviewer:
    def __init__(self, uid: int, task_id: int, review_type: str, review_topic:str,
         criteria_markdown_data: str, review_file_path: str, criteria_file_type, sys_cfg: dict):
        """
        åˆ†ç« èŠ‚è¯„å®¡ç³»ç»Ÿ
        :param uid: ç”¨æˆ·IDï¼Œæ ‡è®°å“ªä¸ªç”¨æˆ·æäº¤çš„ä»»åŠ¡
        :param task_id: ä»»åŠ¡IDï¼Œ æ ‡è®°æ˜¯å“ªä¸ªä»»åŠ¡
        :param review_type: è¯„å®¡ç±»å‹, ä¾‹å¦‚ ä¿¡æ¯ç³»ç»Ÿæ¦‚è¦è®¾è®¡è¯„å®¡
        :param review_topic: è¯„å®¡ä¸»é¢˜ï¼Œ ä¾‹å¦‚ xxxxç³»ç»Ÿæ¦‚è¦æ¶‰åŠè¯„å®¡
        :param criteria_markdown_data: è¯„å®¡æ ‡å‡† markdown æ–‡æœ¬
        :param review_file_path: è¯„å®¡æ–‡ä»¶ markdown è·¯å¾„
        :param criteria_file_type: è¯„å®¡æ ‡å‡†çš„æ–‡ä»¶ç±»å‹
        :param sys_cfg: ç³»ç»Ÿé…ç½®
        """
        self.uid = uid
        self.task_id = task_id
        self.review_type = review_type
        self.review_topic = review_topic
        self.criteria_markdown_data = criteria_markdown_data
        self.criteria_file_type = criteria_file_type
        self.review_file_path = review_file_path
        self.sections_data = []
        self.review_results = []
        self.sys_cfg = sys_cfg



    def review_single_section(self, section_title: str, section_content: str, vdb_dir: str, max_retries: int = 3) -> Dict:
        """
        è¯„å®¡å•ä¸ªç« èŠ‚
        """
        for attempt in range(max_retries):
            try:
                template_name = "section_review_msg"
                template = get_usr_prompt_template(template_name, self.sys_cfg)
                if not template:
                    err_info = f"prompt_template_config_err, {template_name}"
                    raise RuntimeError(err_info)
                reference = get_reference_from_vdb(section_content, vdb_dir, self.sys_cfg['api'])
                logger.info(f"review_single_section_get_reference, {reference}, for_section_content, {section_content}")
                prompt = template.format(
                    review_type = self.review_type,
                    review_topic=self.review_topic,
                    section_title=section_title,
                    section_content=section_content,
                    criteria=self.criteria_markdown_data,
                    reference = reference,
                )
                input_tokens = estimate_tokens(prompt)
                logger.info(f"{self.uid}, input_tokens, {input_tokens}")
                add_input_token_by_uid(self.uid, input_tokens)
                result = self.call_llm_api(prompt)
                output_tokens = estimate_tokens(json.dumps(result))
                logger.info(f"{self.uid}, output_tokens, {output_tokens}")
                add_output_token_by_uid(self.uid, output_tokens)
                # éªŒè¯ç»“æœæ ¼å¼
                PaperReviewer._validate_review_result(result)
                if result.get('issues') and len(result['issues']) > 0:
                    modification_examples = self.generate_modification_examples(
                        section_content,
                        result['issues'],
                        self.criteria_markdown_data
                    )
                    if modification_examples:
                        result['modification_examples'] = modification_examples

                return result

            except Exception as e:
                logger.exception(f"ç¬¬{attempt + 1}æ¬¡è¯„å®¡ç« èŠ‚[{section_title}]å¤±è´¥")
                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè¿”å›åŸºç¡€è¯„å®¡ç»“æœ
                    return self._get_fallback_result(section_title, str(e))
                time.sleep(1)  # é‡è¯•å‰ç­‰å¾…
        logger.error("nothing_return_here, pay attention.")
        return {}

    def call_llm_api(self, prompt: str) -> dict:
        """ç›´æ¥è°ƒç”¨LLM APIï¼Œæ”¯æŒæµå¼è¾“å‡º"""
        key = self.sys_cfg['api']['llm_api_key']
        model = self.sys_cfg['api']['llm_model_name']
        uri = f"{self.sys_cfg['api']['llm_api_uri']}/chat/completions"
        try:
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {key}'
            }

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "user", "content": prompt}
            ]

            # æ„å»ºè¯·æ±‚ä½“
            payload = {
                'model': model,
                'messages': messages,
                'temperature': 0.7,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„JSONè¾“å‡º
                'response_format': {"type": "json_object"}  # å¼ºåˆ¶JSONæ ¼å¼è¾“å‡º
            }
            logger.info(f"start_request, {uri}, {model}, æç¤ºè¯: {prompt[:400]}")
            response = requests.post(
                url=uri,
                headers=headers,
                json=payload,
                timeout=60,
                verify=False,
            )
            logger.info(f"response_status: {response.status_code}")
            if response.status_code == 200:
                result = response.json()
                logger.info(f"LLM å“åº”è§£ææˆåŠŸ")

                # æå–LLMè¿”å›çš„å†…å®¹
                content = result['choices'][0]['message']['content']
                logger.debug(f"LLMè¿”å›çš„åŸå§‹å†…å®¹: {content}")

                try:
                    # å°è¯•ç›´æ¥è§£æJSON
                    if content.strip().startswith('```json'):
                        # å¦‚æœåŒ…å«ä»£ç å—ï¼Œæå–JSONéƒ¨åˆ†
                        json_str = content.strip().replace('```json', '').replace('```', '').strip()
                        parsed_result = json.loads(json_str)
                    else:
                        # ç›´æ¥è§£æ
                        parsed_result = json.loads(content)

                    logger.info(f"æˆåŠŸè§£æLLMè¿”å›çš„JSON: {parsed_result}")
                    return parsed_result

                except json.JSONDecodeError as e:
                    logger.error(f"è§£æLLMè¿”å›çš„JSONå¤±è´¥: {str(e)}")
                    logger.error(f"åŸå§‹å†…å®¹: {content}")
                    # è¿”å›é™çº§ç»“æœ
                    return {
                        "score": 0,
                        "strengths": ["å†…å®¹ç»“æ„å®Œæ•´"],
                        "issues": [f"AIè¯„å®¡è§£æå¤±è´¥: {str(e)}", "å»ºè®®äººå·¥å¤æ ¸"],
                        "suggestions": ["è¯·ä¸“å®¶äººå·¥è¯„å®¡è¯¥ç« èŠ‚"],
                        "risk_level": "æœªçŸ¥"
                    }
            else:
                error_msg = f"LLM APIè°ƒç”¨å¤±è´¥:{uri}, {response.status_code} - {response.text}"
                logger.error(error_msg)
                return {
                    "score": 0,
                    "strengths": ["ç« èŠ‚ç»“æ„å®Œæ•´"],
                    "issues": [f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code}- {response.text}"],
                    "suggestions": ["è¯·ä¸“å®¶äººå·¥è¯„å®¡è¯¥ç« èŠ‚"],
                    "risk_level": "æœªçŸ¥"
                }
        except Exception as e:
            error_msg = f"LLM APIè°ƒç”¨å¼‚å¸¸: {uri}, {str(e)}"
            logger.error(error_msg)
            return {
                "score": 0,
                "strengths": ["ç« èŠ‚ç»“æ„å®Œæ•´"],
                "issues": [f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"],
                "suggestions": ["è¯·ä¸“å®¶äººå·¥è¯„å®¡è¯¥ç« èŠ‚"],
                "risk_level": "æœªçŸ¥"
            }

    @staticmethod
    def _validate_review_result(result: Dict):
        """éªŒè¯è¯„å®¡ç»“æœæ ¼å¼"""
        required_fields = ['score', 'strengths', 'issues', 'suggestions']
        for field in required_fields:
            if field not in result:
                raise ValueError(f"è¯„å®¡ç»“æœç¼ºå°‘å¿…è¦å­—æ®µ: {field}")

        if not isinstance(result['score'], int) or not (0 <= result['score'] <= 100):
            raise ValueError("è¯„åˆ†å¿…é¡»åœ¨0-100ä¹‹é—´")

    @staticmethod
    def _get_fallback_result(section_title: str, error_msg: str) -> Dict:
        """è·å–é™çº§è¯„å®¡ç»“æœ"""
        return {
            "score": 60,
            "strengths": ["ç« èŠ‚ç»“æ„å®Œæ•´"],
            "issues": [f"è¯„å®¡è¿‡ç¨‹ä¸­å‡ºç°æŠ€æœ¯é—®é¢˜: {error_msg}", "å»ºè®®äººå·¥å¤æ ¸"],
            "suggestions": ["è¯·ä¸“å®¶äººå·¥è¯„å®¡è¯¥ç« èŠ‚"],
            "risk_level": "æœªçŸ¥",
            "review_failed": True
        }

    def review_whole_report(self, section_results: List[Dict]) -> Dict:
        """
        æ ¹æ®å„ä¸ªç« èŠ‚çš„è¯„å®¡ç»“æœï¼Œè¿›è¡Œæ•´ä½“è¯„å®¡
        """
        try:
            # ç”Ÿæˆå„ç« èŠ‚æ¦‚è¦
            section_summaries = []
            for result in section_results:
                # å¤„ç† issuesï¼Œç¡®ä¿æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                main_issues = []
                if isinstance(result.get('issues'), list) and len(result['issues']) > 0:
                    for issue in result['issues'][:2]:
                        if isinstance(issue, dict):
                            # ä»å­—å…¸ä¸­æå–æè¿°
                            issue_desc = issue.get('description', str(issue))
                            issue_loc = issue.get('location', '')
                            main_issues.append(f"{issue_loc}: {issue_desc}" if issue_loc else issue_desc)
                        else:
                            main_issues.append(str(issue))

                summary = {
                    'title': result['section_title'],
                    'score': result['score'],
                    'main_issues': main_issues,
                    'risk_level': result.get('risk_level', 'æœªçŸ¥')
                }
                section_summaries.append(summary)
            logger.debug(f"section_summaries {section_summaries}")
            template_name = "paper_review_msg"
            template = get_usr_prompt_template(template_name, self.sys_cfg)
            if not template:
                err_info = f"prompt_template_config_err, {template_name}"
                raise RuntimeError(err_info)
            prompt = template.format(
                review_type = self.review_type,
                review_topic = self.review_topic,
                section_summary = json.dumps(section_summaries, ensure_ascii=False, indent=2),
                criteria = self.criteria_markdown_data,
            )
            input_tokens = estimate_tokens(prompt)
            logger.info(f"{self.uid}, input_tokens, {input_tokens}")
            add_input_token_by_uid(self.uid, input_tokens)
            overall_result = self.call_llm_api(prompt)
            output_tokens = estimate_tokens(json.dumps(overall_result))
            logger.info(f"{self.uid}, output_tokens, {output_tokens}")
            add_output_token_by_uid(self.uid, output_tokens)
            # æ·»åŠ ç»“æœéªŒè¯å’Œé™çº§å¤„ç†
            if not overall_result or 'overall_score' not in overall_result:
                logger.warning("æ•´ä½“è¯„å®¡è¿”å›ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é™çº§ç»“æœ")
                return PaperReviewer._get_fallback_overall_result(section_results)
            return overall_result

        except Exception as e:
            logger.error(f"æ•´ä½“è¯„å®¡å¤±è´¥: {str(e)}")
            return PaperReviewer._get_fallback_overall_result(section_results)

    @staticmethod
    def _get_fallback_overall_result(section_results: List[Dict]) -> Dict:
        """è·å–æ•´ä½“è¯„å®¡çš„é™çº§ç»“æœ"""
        try:
            # è®¡ç®—å¹³å‡åˆ†
            avg_score = sum(r['score'] for r in section_results) // len(section_results) if section_results else 60

            return {
                "overall_score": avg_score,
                "overall_strengths": ["æŠ¥å‘Šç»“æ„å®Œæ•´"],
                "overall_issues": ["æ•´ä½“è¯„å®¡è¿‡ç¨‹å‡ºç°æŠ€æœ¯é—®é¢˜ï¼Œå»ºè®®äººå·¥å¤æ ¸"],
                "key_recommendations": ["å»ºè®®ä¸“å®¶å¯¹æ•´ç¯‡æŠ¥å‘Šè¿›è¡Œäººå·¥è¯„å®¡"],
                "review_summary": f"æŠ¥å‘Šå„ç« èŠ‚å¹³å‡è¯„åˆ†ä¸º{avg_score}åˆ†ã€‚ç”±äºæŠ€æœ¯åŸå› ï¼Œæ•´ä½“è¯„å®¡æœªèƒ½å®Œæˆï¼Œå»ºè®®ä¸“å®¶äººå·¥å¤æ ¸æ•´ç¯‡æŠ¥å‘Šã€‚"
            }
        except Exception as e:
            logger.error(f"ç”Ÿæˆé™çº§æ•´ä½“ç»“æœå¤±è´¥: {str(e)}")
            return {
                "overall_score": 60,
                "overall_strengths": [],
                "overall_issues": ["è¯„å®¡ç³»ç»Ÿå‡ºç°æŠ€æœ¯æ•…éšœ"],
                "key_recommendations": ["è¯·ä¸“å®¶è¿›è¡Œå®Œæ•´äººå·¥è¯„å®¡"],
                "review_summary": "è¯„å®¡ç³»ç»Ÿå‡ºç°æŠ€æœ¯é—®é¢˜ï¼Œå»ºè®®ä¸“å®¶è¿›è¡Œå®Œæ•´äººå·¥è¯„å®¡ã€‚"
            }

    def generate_final_report(self, section_results: List[Dict], overall_result: Dict) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆè¯„å®¡æŠ¥å‘Šçš„å†…å®¹
        """
        try:
            report_content = f"""# ã€ {self.review_topic} ã€‘ è¯„å®¡æŠ¥å‘Š

## è¯„å®¡æ¦‚è¿°
- è¯„å®¡æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
- æ•´ä½“è¯„åˆ†: {overall_result['overall_score']}/100
- è¯„å®¡ç»“è®º: {'é€šè¿‡' if overall_result['overall_score'] >= 60 else 'éœ€è¦ä¿®æ”¹'}

## æ•´ä½“è¯„ä»·
{overall_result['review_summary']}

### ä¸»è¦ä¼˜åŠ¿
{chr(10).join(f"- {strength}" for strength in overall_result['overall_strengths'])}

### ä¸»è¦é—®é¢˜  
{chr(10).join(f"- {issue}" for issue in overall_result['overall_issues'])}

### å…³é”®å»ºè®®
{chr(10).join(f"- {recommendation}" for recommendation in overall_result['key_recommendations'])}

## å„ç« èŠ‚è¯„å®¡ç»“æœ
"""
            # æ”¶é›†æ‰€æœ‰ä¿®æ”¹ç¤ºä¾‹
            all_modification_examples = []
            for section_result in section_results:
                # åŸºæœ¬ç« èŠ‚ä¿¡æ¯
                report_content += f"""
### {section_result['section_title']}
- **è¯„åˆ†**: {section_result['score']}/100
- **é£é™©ç­‰çº§**: {section_result.get('risk_level', 'æœªçŸ¥')}

#### ä¼˜ç‚¹
{chr(10).join(f"  - {strength}" for strength in section_result['strengths'])}

#### é—®é¢˜
"""
                # æ ¼å¼åŒ–é—®é¢˜åˆ—è¡¨
                for issue in section_result['issues']:
                    if isinstance(issue, dict):
                        issue_desc = issue.get('description', str(issue))
                        issue_loc = issue.get('location', 'æœªçŸ¥ä½ç½®')
                        report_content += f"  - **{issue_loc}**: {issue_desc}\n"
                        if issue.get('severity'):
                            report_content += f"    - ä¸¥é‡ç¨‹åº¦: {issue['severity']}\n"
                    else:
                        report_content += f"  - {issue}\n"

                report_content += "\n#### æ”¹è¿›å»ºè®®\n"

                # æ ¼å¼åŒ–å»ºè®®åˆ—è¡¨
                for suggestion in section_result['suggestions']:
                    if isinstance(suggestion, dict):
                        report_content += f"  - **å»ºè®®**: {suggestion.get('recommendation', suggestion)}\n"
                        if suggestion.get('reason'):
                            report_content += f"    - ç†ç”±: {suggestion['reason']}\n"
                        if suggestion.get('example_before') and suggestion.get('example_after'):
                            report_content += f"    - ä¿®æ”¹å‰: {suggestion['example_before']}\n"
                            report_content += f"    - ä¿®æ”¹å: {suggestion['example_after']}\n"
                    else:
                        report_content += f"  - {suggestion}\n"

                # æ”¶é›†å…·ä½“ä¿®æ”¹ç¤ºä¾‹
                if 'modification_examples' in section_result:
                    for example in section_result['modification_examples']:
                        if isinstance(example, dict):
                            all_modification_examples.append({
                                'section': section_result['section_title'],
                                'example': example
                            })
                            # åœ¨ç« èŠ‚ä¸­æ˜¾ç¤ºç¤ºä¾‹
                            report_content += f"\n#### å…·ä½“ä¿®æ”¹ç¤ºä¾‹\n"
                            report_content += f"**åŸæ–‡**: {example.get('original_text', '')}\n"
                            report_content += f"**ä¿®æ”¹å»ºè®®**: {example.get('modified_text', '')}\n"
                            report_content += f"**è¯´æ˜**: {example.get('explanation', '')}\n\n"

            # æ·»åŠ ä¸“é—¨çš„ä¿®æ”¹ç¤ºä¾‹éƒ¨åˆ†
            if all_modification_examples:
                report_content += "\n## ğŸ“ å…·ä½“ä¿®æ”¹ç¤ºä¾‹æ±‡æ€»\n\n"
                report_content += "ä»¥ä¸‹ä¸ºå„ç« èŠ‚çš„å…·ä½“ä¿®æ”¹ç¤ºä¾‹ï¼Œå¯ç›´æ¥åº”ç”¨äºæ–‡æ¡£ä¿®æ”¹ï¼š\n\n"

                for i, item in enumerate(all_modification_examples, 1):
                    example = item['example']
                    report_content += f"### ç¤ºä¾‹ {i}: {item['section']}\n\n"
                    report_content += f"**é—®é¢˜ä½ç½®**: {example.get('location', 'è¯¥ç« èŠ‚')}\n\n"
                    report_content += f"**åŸæ–‡**:\n```\n{example.get('original_text', '')}\n```\n\n"
                    report_content += f"**ä¿®æ”¹å»ºè®®**:\n```\n{example.get('modified_text', '')}\n```\n\n"
                    report_content += f"**ä¿®æ”¹è¯´æ˜**: {example.get('explanation', '')}\n\n"
                    report_content += "---\n\n"

            report_content += """
    ## è¯„å®¡è¯´æ˜
    æœ¬è¯„å®¡æŠ¥å‘Šç”±AIç³»ç»Ÿç”Ÿæˆï¼ŒåŒ…å«å…·ä½“çš„ä¿®æ”¹ç¤ºä¾‹ï¼Œå¯ç›´æ¥å‚è€ƒè¿›è¡Œæ–‡æ¡£ä¿®è®¢ã€‚
    å»ºè®®ç»“åˆä¸“å®¶äººå·¥è¯„å®¡æœ€ç»ˆç¡®å®šã€‚
    """

            return report_content

        except Exception as e:
            logger.error(f"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return f"# è¯„å®¡æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"

    def execute_review(self) -> str:
        """
        æ‰§è¡Œå®Œæ•´çš„è¯„å®¡æµç¨‹ï¼Œè¿”å›ç”Ÿæˆçš„æŠ¥å‘Šæ–‡æœ¬
        """
        try:
            logger.info("å¼€å§‹æ‰§è¡Œæ–‡æ¡£è¯„å®¡æµç¨‹")

            # 1. è§£æç›®å½•ç»“æ„
            catalogue = get_md_file_catalogue(self.review_file_path)
            if not catalogue:
                info = f"æ— æ³•è§£ææ–‡æ¡£ç›®å½•ç»“æ„, {self.review_file_path}"
                raise ValueError(info)
            logger.info(f"æ–‡æ¡£ç›®å½•ï¼Œ{catalogue}")
            update_process_info(self.uid, self.task_id, "å¼€å§‹è§£æç« èŠ‚å†…å®¹...")

            # 2. æå–ç« èŠ‚å†…å®¹
            # extract_sections_content è¿”å›çš„æ•°æ®æ ¼å¼ï¼š [{"heading1->header2" : ["content_part1 under heading2", "content_part2 under heading2"]}]
            self.sections_data = extract_sections_content(self.review_file_path, catalogue,
                                                          max_content_length=MAX_SECTION_LENGTH)
            if not self.sections_data:
                raise ValueError("æ— æ³•æå–ç« èŠ‚å†…å®¹")
            logger.debug(f"ç« èŠ‚å†…å®¹ï¼Œ{self.sections_data}")

            # è®¡ç®—æ€»ç« èŠ‚æ•°ï¼ˆåŒ…æ‹¬åˆ†å‰²åçš„éƒ¨åˆ†ï¼‰
            total_sections = 0
            for section in self.sections_data:
                for title, content_parts in section.items():
                    total_sections += len(content_parts)

            doc_info = get_doc_info(self.task_id)
            if not doc_info or not doc_info[0]:
                info = f"no_doc_info_found_for_task_id ,{self.task_id}"
                raise RuntimeError(info)
            vdb_id = doc_info[0]['vdb_id']
            vdb_info = VdbMeta.get_vdb_info_by_id(vdb_id)
            if vdb_info:
                my_vector_db_dir = f"{VDB_PREFIX}{vdb_info[0]['uid']}_{vdb_id}"
            else:
                my_vector_db_dir = ""
            logger.info(f"my_vector_db_dir {my_vector_db_dir}")
            logger.info(f"å¼€å§‹é€ç« èŠ‚è¯„å®¡ï¼Œå…± {len(self.sections_data)} ä¸ªåŸå§‹ç« èŠ‚ï¼Œ{total_sections} ä¸ªå†…å®¹éƒ¨åˆ†")

            # 3. é€ç« èŠ‚è¯„å®¡
            processed_parts = 0
            for i, section in enumerate(self.sections_data):
                # æ¯ä¸ªsectionæ˜¯ä¸€ä¸ªå­—å…¸ï¼š{"heading1->header2": ["content_part1", "content_part2", ...]}
                for section_title, content_parts in section.items():
                    logger.info(
                        f"è¯„å®¡ç« èŠ‚ {i + 1}/{len(self.sections_data)}: {section_title} (åŒ…å«{len(content_parts)}ä¸ªéƒ¨åˆ†)")

                    # å¯¹æ¯ä¸ªå†…å®¹éƒ¨åˆ†è¿›è¡Œè¯„å®¡
                    for part_index, content_part in enumerate(content_parts):
                        processed_parts += 1
                        current_percent = min(95.0, round(processed_parts / total_sections * 100, 1))

                        part_description = f"ç¬¬{part_index + 1}éƒ¨åˆ†" if len(content_parts) > 1 else "å®Œæ•´å†…å®¹"
                        process_info = f"æ­£åœ¨è¯„å®¡ç« èŠ‚ {section_title} çš„{part_description} ({processed_parts}/{total_sections})"
                        update_process_info(self.uid, self.task_id, process_info, current_percent)

                        # æ„å»ºå®Œæ•´çš„éƒ¨åˆ†æ ‡é¢˜ï¼ˆåŒ…å«éƒ¨åˆ†ä¿¡æ¯ï¼‰
                        full_section_title = f"{section_title} [ç¬¬{part_index + 1}éƒ¨åˆ†]" if len(
                            content_parts) > 1 else section_title

                        logger.debug(f"è¯„å®¡å†…å®¹éƒ¨åˆ†: {full_section_title}, é•¿åº¦: {len(content_part)}")

                        section_result = self.review_single_section(full_section_title, content_part, my_vector_db_dir)
                        section_result['section_title'] = section_title  # ä¿å­˜åŸå§‹æ ‡é¢˜
                        section_result['part_index'] = part_index
                        section_result['total_parts'] = len(content_parts)
                        self.review_results.append(section_result)
                        logger.info(f"ç« èŠ‚[{full_section_title}]è¯„å®¡å®Œæˆï¼Œè¯„åˆ†: {section_result['score']}")

            # 4. åˆå¹¶åŒä¸€ç« èŠ‚çš„å¤šä¸ªéƒ¨åˆ†ç»“æœ
            merged_results = PaperReviewer._merge_section_results(self.review_results)
            # 5. æ•´ä½“è¯„å®¡
            logger.info("æ€»ç»“è¯„å®¡æ„è§")
            update_process_info(self.uid, self.task_id, "å¼€å§‹æ€»ç»“è¯„å®¡æ„è§")
            overall_result = self.review_whole_report(merged_results)

            # 6. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            logger.info("ç”Ÿæˆæœ€ç»ˆè¯„å®¡æŠ¥å‘Š")
            final_report = self.generate_final_report(merged_results, overall_result)
            if FileType.XLSX.value == self.criteria_file_type:
                update_process_info(self.uid, self.task_id, "ç”Ÿæˆæ ¼å¼åŒ–çš„è¯„å®¡æŠ¥å‘Š")
                logger.debug(f"fill_all_formatted_markdown_report_with_final_report\n{final_report}")
                formatted_report = self.fill_all_formatted_markdown_report_with_final_report(final_report)
            else:
                formatted_report = final_report
            logger.info("æ–‡æ¡£è¯„å®¡æµç¨‹å®Œæˆ")
            return formatted_report

        except Exception as e:
            logger.exception(f"è¯„å®¡æµç¨‹æ‰§è¡Œå¤±è´¥")
            return f"# è¯„å®¡è¿‡ç¨‹å‡ºç°é”™è¯¯\n\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯·æ£€æŸ¥æ–‡æ¡£æ ¼å¼æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚"

    @staticmethod
    def _merge_section_results(all_results: List[Dict]) -> List[Dict]:
        """
        åˆå¹¶åŒä¸€ç« èŠ‚çš„å¤šä¸ªéƒ¨åˆ†è¯„å®¡ç»“æœ

        Args:
            all_results: æ‰€æœ‰éƒ¨åˆ†çš„è¯„å®¡ç»“æœ

        Returns:
            åˆå¹¶åçš„ç« èŠ‚è¯„å®¡ç»“æœ
        """
        merged_results = {}

        for result in all_results:
            section_title = result['section_title']
            part_index = result.get('part_index', 0)
            total_parts = result.get('total_parts', 1)

            if section_title not in merged_results:
                # åˆå§‹åŒ–ç« èŠ‚ç»“æœ
                merged_results[section_title] = {
                    'section_title': section_title,
                    'scores': [],
                    'strengths': [],
                    'issues': [],
                    'suggestions': [],
                    'risk_levels': [],
                    'part_count': total_parts
                }

            # æ”¶é›†å„éƒ¨åˆ†ç»“æœ
            merged_results[section_title]['scores'].append(result['score'])
            merged_results[section_title]['strengths'].extend(result['strengths'])

            # å¤„ç† issuesï¼šå¦‚æœæ˜¯å­—å…¸åˆ™è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å…ƒç»„
            for issue in result['issues']:
                if isinstance(issue, dict):
                    # å°†å­—å…¸è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å…ƒç»„
                    issue_tuple = tuple(sorted(issue.items()))
                    merged_results[section_title]['issues'].append(issue_tuple)
                else:
                    merged_results[section_title]['issues'].append(issue)

            # å¤„ç† suggestionsï¼šå¦‚æœæ˜¯å­—å…¸åˆ™è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å…ƒç»„
            for suggestion in result['suggestions']:
                if isinstance(suggestion, dict):
                    # å°†å­—å…¸è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å…ƒç»„
                    suggestion_tuple = tuple(sorted(suggestion.items()))
                    merged_results[section_title]['suggestions'].append(suggestion_tuple)
                else:
                    merged_results[section_title]['suggestions'].append(suggestion)

            merged_results[section_title]['risk_levels'].append(result.get('risk_level', 'æœªçŸ¥'))

        # ç”Ÿæˆæœ€ç»ˆåˆå¹¶ç»“æœ
        final_results = []
        for section_title, data in merged_results.items():
            # è®¡ç®—å¹³å‡åˆ†
            avg_score = sum(data['scores']) // len(data['scores'])

            # å»é‡å¹¶ä¿ç•™é‡è¦ä¿¡æ¯
            unique_strengths = list(dict.fromkeys(data['strengths']))  # ä¿æŒé¡ºåºå»é‡

            # å¤„ç† issuesï¼šå…ˆå»é‡ï¼Œç„¶åè¿˜åŸæ ¼å¼
            unique_issues = []
            seen_issues = set()
            for issue in data['issues']:
                if isinstance(issue, tuple):
                    # å…ƒç»„æ ¼å¼çš„issueï¼ˆåŸå§‹ä¸ºå­—å…¸ï¼‰
                    if issue not in seen_issues:
                        seen_issues.add(issue)
                        # å°†å…ƒç»„è½¬æ¢å›å­—å…¸
                        unique_issues.append(dict(issue))
                else:
                    # å­—ç¬¦ä¸²æ ¼å¼çš„issue
                    if issue not in seen_issues:
                        seen_issues.add(issue)
                        unique_issues.append(issue)

            # å¤„ç† suggestionsï¼šå…ˆå»é‡ï¼Œç„¶åè¿˜åŸæ ¼å¼
            unique_suggestions = []
            seen_suggestions = set()
            for suggestion in data['suggestions']:
                if isinstance(suggestion, tuple):
                    # å…ƒç»„æ ¼å¼çš„suggestionï¼ˆåŸå§‹ä¸ºå­—å…¸ï¼‰
                    if suggestion not in seen_suggestions:
                        seen_suggestions.add(suggestion)
                        # å°†å…ƒç»„è½¬æ¢å›å­—å…¸
                        unique_suggestions.append(dict(suggestion))
                else:
                    # å­—ç¬¦ä¸²æ ¼å¼çš„suggestion
                    if suggestion not in seen_suggestions:
                        seen_suggestions.add(suggestion)
                        unique_suggestions.append(suggestion)

            # ç¡®å®šä¸»è¦é£é™©ç­‰çº§ï¼ˆå–æœ€ä¸¥é‡çš„ï¼‰
            risk_levels = data['risk_levels']
            risk_priority = {'é«˜': 3, 'ä¸­': 2, 'ä½': 1, 'æœªçŸ¥': 0}
            main_risk_level = max(risk_levels, key=lambda x: risk_priority.get(x, 0))

            final_result = {
                'section_title': section_title,
                'score': avg_score,
                'strengths': unique_strengths[:5],  # é™åˆ¶æ•°é‡ï¼Œå–å‰5ä¸ª
                'issues': unique_issues[:10],  # é™åˆ¶æ•°é‡ï¼Œå–å‰10ä¸ª
                'suggestions': unique_suggestions[:5],  # é™åˆ¶æ•°é‡ï¼Œå–å‰5ä¸ª
                'risk_level': main_risk_level,
                'original_parts_count': data['part_count']
            }

            final_results.append(final_result)

            logger.info(f"åˆå¹¶ç« èŠ‚[{section_title}]ç»“æœ: å¹³å‡åˆ†{avg_score}, åŸå§‹éƒ¨åˆ†æ•°{data['part_count']}")

        return final_results

    def fill_all_formatted_markdown_report_with_final_report(self, final_report_txt: str) -> str:
        """
        ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å°†æœ€ç»ˆè¯„å®¡ç»“æœè‡ªåŠ¨å¡«å†™åˆ°æ ‡å‡†æ ¼å¼çš„è¯„å®¡è¡¨æ ¼ä¸­

        Args:
            final_report_txt: ç”Ÿæˆçš„æœ€ç»ˆè¯„å®¡æŠ¥å‘Šæ–‡æœ¬

        Returns:
            å¡«å……åçš„æ ¼å¼åŒ–è¯„å®¡æŠ¥å‘Šæ–‡æœ¬
        """
        split_md_list = split_md_content_with_catalogue(self.criteria_markdown_data)
        logger.debug(f"split_md_list, {split_md_list}")
        all_txt = ""
        for md in split_md_list:
            single_title = md['title']
            single_criteria = md['content']
            logger.debug(f"get_single_criteria_md, \n{single_criteria}")
            single_report = self.fill_md_table(final_report_txt, single_title, single_criteria)
            all_txt = all_txt  + "\n\n" + single_report
        return all_txt

    def fill_md_table(self, conclusion: str, single_criteria_title: str, single_criteria: str) -> str:
        """
        ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å°†æœ€ç»ˆè¯„å®¡ç»“æœè‡ªåŠ¨å¡«å†™åˆ°æ ‡å‡†æ ¼å¼çš„è¯„å®¡è¡¨æ ¼ä¸­

        Args:
            conclusion: ç”Ÿæˆçš„æœ€ç»ˆè¯„å®¡ç»“è®º
            single_criteria_title: è¯„å®¡æ ‡å‡†ä¸­çš„ä¸€ä¸ªè¡¨æ ¼çš„æ ‡é¢˜
            single_criteria: è¯„å®¡æ ‡å‡†ä¸­çš„ä¸€ä¸ªè¡¨æ ¼

        Returns:
            å¡«å……åçš„æ ¼å¼åŒ–è¯„å®¡æŠ¥å‘Šæ–‡æœ¬
        """
        template_name = "fill_md_table_msg"
        template = get_usr_prompt_template(template_name, self.sys_cfg)
        if not template:
            err_info = f"prompt_template_config_err, {template_name}"
            raise RuntimeError(err_info)
        prompt = template.format(
            review_type = self.review_type,
            review_topic = self.review_topic,
            criteria=single_criteria,
            conclusion=conclusion,
        )
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨LLMå°†è¯„å®¡ç»“æœå¡«å……åˆ°æ ‡å‡†æ ¼å¼è¡¨æ ¼ä¸­, title={single_criteria_title}")
            # è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹API
            start_time = time.time()
            filled_report = self.call_llm_api_for_formatting(prompt)
            end_time = time.time()
            execution_time = end_time - start_time
            # éªŒè¯è¿”å›ç»“æœ
            if filled_report and self._is_valid_filled_report(filled_report):
                logger.info(f"æˆåŠŸç”Ÿæˆæ ¼å¼åŒ–è¯„å®¡æŠ¥å‘Š, è€—æ—¶: {execution_time:.2f} ç§’, title={single_criteria_title}")
                return filled_report
            else:
                logger.warning("LLMè¿”å›çš„æ ¼å¼åŒ–æŠ¥å‘Šä¸å®Œæ•´ï¼Œè¿”å›åŸå§‹æŠ¥å‘Š")
                return conclusion

        except Exception as e:
            logger.error(f"ä½¿ç”¨LLMå¡«å……æ ¼å¼åŒ–æŠ¥å‘Šå¤±è´¥: {str(e)}")
        # å¦‚æœå¡«å……å¤±è´¥ï¼Œè¿”å›åŸå§‹æŠ¥å‘Š
        return conclusion

    def call_llm_api_for_formatting(self, prompt: str, max_retries: int = 2) -> str:
        """ä¸“é—¨ç”¨äºæ ¼å¼åŒ–æŠ¥å‘Šçš„å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨ï¼Œå¢åŠ é‡è¯•æœºåˆ¶"""
        key = self.sys_cfg['api']['llm_api_key']
        model = self.sys_cfg['api']['llm_model_name']
        uri = f"{self.sys_cfg['api']['llm_api_uri']}/chat/completions"

        for attempt in range(max_retries):
            try:
                headers = {
                    'Content-Type': 'application/json',
                    'Authorization': f'Bearer {key}'
                }

                # æ„å»ºæ¶ˆæ¯
                messages = [
                    {"role": "system",
                     "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ {self.review_topic} æ–‡æ¡£è¯„å®¡ä¸“å®¶ï¼Œæ“…é•¿å°†è¯„å®¡ç»“æœæŒ‰ç…§æ ‡å‡†è¡¨æ ¼æ ¼å¼è¿›è¡Œæ•´ç†å’Œå¡«å†™ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„è¡¨æ ¼æ ¼å¼è¦æ±‚è¿›è¡Œæ“ä½œã€‚"},
                    {"role": "user", "content": prompt}
                ]

                # æ„å»ºè¯·æ±‚ä½“ - ä¼˜åŒ–å‚æ•°
                payload = {
                    'model': model,
                    'messages': messages,
                    'temperature': 0.1,
                    'max_tokens': 8192,
                    'stream': False  # ç¡®ä¿éæµå¼å“åº”
                }

                logger.info(f"å¼€å§‹è°ƒç”¨LLMè¿›è¡ŒæŠ¥å‘Šæ ¼å¼åŒ– (ç¬¬{attempt + 1}æ¬¡å°è¯•)")

                # åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
                timeout = 300 if attempt == 0 else 600  # ç¬¬ä¸€æ¬¡180ç§’ï¼Œé‡è¯•æ—¶300ç§’

                response = requests.post(
                    url=uri,
                    headers=headers,
                    json=payload,
                    timeout=timeout,
                    verify=False,
                )

                logger.info(f"LLMæ ¼å¼åŒ–å“åº”çŠ¶æ€: {response.status_code}")

                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']

                    # æ¸…ç†è¿”å›å†…å®¹
                    if content.strip().startswith('```'):
                        lines = content.strip().split('\n')
                        if lines[0].startswith('```'):
                            lines = lines[1:]
                        if lines and lines[-1].startswith('```'):
                            lines = lines[:-1]
                        content = '\n'.join(lines).strip()

                    logger.info(f"æˆåŠŸè·å–LLMæ ¼å¼åŒ–çš„æŠ¥å‘Šï¼Œé•¿åº¦: {len(content)}")
                    return content

                else:
                    error_msg = f"LLMæ ¼å¼åŒ–APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    if attempt == max_retries - 1:
                        return ""
                    time.sleep(2)  # é‡è¯•å‰ç­‰å¾…

            except requests.exceptions.Timeout:
                logger.warning(f"ç¬¬{attempt + 1}æ¬¡è°ƒç”¨LLM APIè¶…æ—¶")
                if attempt == max_retries - 1:
                    logger.error("æ‰€æœ‰é‡è¯•å°è¯•å‡è¶…æ—¶")
                    return ""
                time.sleep(3)  # è¶…æ—¶åç­‰å¾…æ›´é•¿æ—¶é—´

            except Exception as e:
                error_msg = f"LLMæ ¼å¼åŒ–APIè°ƒç”¨å¼‚å¸¸ (ç¬¬{attempt + 1}æ¬¡): {str(e)}"
                logger.error(error_msg)
                if attempt == max_retries - 1:
                    return ""
                time.sleep(2)

        return ""

    def generate_modification_examples(self, section_content: str, issues: List, criteria: str) -> List[Dict]:
        """
        é’ˆå¯¹é—®é¢˜ç”Ÿæˆå…·ä½“çš„ä¿®æ”¹ç¤ºä¾‹
        """
        if not issues:
            return []

        examples = []
        for issue in issues:
            if isinstance(issue, dict):
                issue_desc = issue.get('description', '')
                issue_loc = issue.get('location', '')
            else:
                issue_desc = issue
                issue_loc = ''

            # è°ƒç”¨LLMç”Ÿæˆä¿®æ”¹ç¤ºä¾‹
            example = self._generate_single_modification_example(
                section_content, issue_desc, issue_loc, criteria
            )
            if example:
                examples.append(example)

        return examples

    def _generate_single_modification_example(self, content: str, issue: str, location: str, criteria: str) -> Dict:
        """
        ç”Ÿæˆå•ä¸ªé—®é¢˜çš„ä¿®æ”¹ç¤ºä¾‹
        """
        try:
            template_name = "modification_example_msg"
            template = get_usr_prompt_template(template_name, self.sys_cfg)
            if not template:
                info = f"æœªæ‰¾åˆ°ä¿®æ”¹ç¤ºä¾‹æ¨¡æ¿ {template_name}ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†"
                logger.warning(info)
                raise RuntimeError(info)

            prompt = template.format(
                review_type=self.review_type,
                review_topic=self.review_topic,
                original_content=content[:2000],  # é™åˆ¶é•¿åº¦
                issue_description=issue,
                issue_location=location,
                criteria_requirement=criteria[:1000]
            )

            result = self.call_llm_api(prompt)

            # éªŒè¯ç»“æœæ ¼å¼
            if isinstance(result, dict) and 'original_excerpt' in result:
                return {
                    'original_text': result['original_excerpt'],
                    'modified_text': result.get('modified_version', ''),
                    'explanation': result.get('modification_rationale', ''),
                    'standard': result.get('applicable_standard', ''),
                    'location': location
                }

        except Exception as e:
            logger.error(f"ç”Ÿæˆä¿®æ”¹ç¤ºä¾‹å¤±è´¥: {str(e)}")

        return None


    @staticmethod
    def _is_valid_filled_report(report: str) -> bool:
        """
        éªŒè¯å¡«å……åçš„æŠ¥å‘Šæ˜¯å¦æœ‰æ•ˆ
        """
        if not report or len(report.strip()) < 100:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼ç‰¹å¾
        table_indicators = ['|', '---', 'è¯„åˆ†', 'å¾—åˆ†', 'è¯„å®¡æ„è§']
        indicators_found = sum(1 for indicator in table_indicators if indicator in report)

        # å¦‚æœæ‰¾åˆ°è‡³å°‘2ä¸ªè¡¨æ ¼ç‰¹å¾ï¼Œè®¤ä¸ºæŠ¥å‘Šæœ‰æ•ˆ
        return indicators_found >= 2

def start_ai_review(uid:int, task_id: int, review_type: str, review_topic:str,
        criteria_markdown_data: str, review_file_path: str, criteria_file_type: int , sys_cfg: dict) -> str:
    """
    :param uid: ç”¨æˆ·ID
    :param task_id: å½“å‰ä»»åŠ¡ID
    :param review_type è¯„å®¡ç±»å‹
    :param review_topic: è¯„å®¡ä¸»é¢˜
    :param criteria_markdown_data: è¯„å®¡æ ‡å‡†å’Œè¦æ±‚markdown æ–‡æœ¬
    :param review_file_path: è¢«è¯„å®¡çš„ææ–™æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    :param criteria_file_type: è¯„å®¡æ ‡å‡†æ–‡ä»¶çš„ç±»å‹
    :param sys_cfgï¼š ç³»ç»Ÿé…ç½®ä¿¡æ¯
    æ ¹æ®è¯„å®¡æ ‡å‡†æ–‡æœ¬å’Œè¯„å®¡ææ–™ç”Ÿæˆè¯„å®¡æŠ¥å‘Š
    """
    try:
        # åˆ›å»ºè¯„å®¡å™¨å¹¶æ‰§è¡Œè¯„å®¡
        reviewer = PaperReviewer(uid, task_id, review_type, review_topic, criteria_markdown_data, review_file_path, criteria_file_type, sys_cfg)
        review_report = reviewer.execute_review()
        return review_report

    except Exception as e:
        logger.error(f"AIè¯„å®¡ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"è¯„å®¡æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"


def generate_review_report(uid: int, task_id: int, doc_type: str, review_topic: str,
                           criteria_file: str, paper_file: str, criteria_file_type: int, sys_cfg: dict):
    """
    ç”Ÿæˆè¯„å®¡æŠ¥å‘Š
    :param uid: ç”¨æˆ·ID
    :param doc_type: è¯„å®¡çš„æ–‡æ¡£å†…å®¹ï¼Œä¾‹å¦‚å¯è¡Œæ€§ç ”ç©¶æŠ¥å‘Šï¼Œæ¦‚è¦è®¾è®¡ï¼Œ AIåº”ç”¨è®¾è®¡ç­‰
    :param review_topic : è¯„å®¡çš„ä¸»é¢˜ï¼Œ ä¾‹å¦‚å…³äºxxxxçš„è¯„å®¡
    :param task_id: ä»»åŠ¡ID
    :param criteria_file: è¯„å®¡æ ‡å‡†æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    :param paper_file: è¯„å®¡ææ–™æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    :param criteria_file_type: è¯„å®¡æ ‡å‡†æ–‡ä»¶çš„ç±»å‹
    :param sys_cfg: ç³»ç»Ÿé…ç½®ä¿¡æ¯
    """
    logger.info(f"{uid}, {task_id},doc_type: {doc_type}, doc_title: {review_topic}, "
                f"criteria_file: {criteria_file}, review_file: {paper_file}")
    try:
        update_process_info(uid, task_id, "å¼€å§‹è§£æè¯„å®¡æ ‡å‡†...")
        # è·å–è¯„å®¡æ ‡å‡†çš„æ–‡ä»¶å†…å®¹ï¼Œæ ¼å¼ä¸º Markdown
        criteria_markdown_data = get_md_file_content(criteria_file)
        update_process_info(uid, task_id, "å¼€å§‹åˆ†æè¯„å®¡ææ–™...")

        # è°ƒç”¨AIè¯„å®¡ç”Ÿæˆ
        review_result = start_ai_review(uid, task_id, doc_type, review_topic,
            criteria_markdown_data, paper_file, criteria_file_type, sys_cfg)

        doc_info = get_doc_info(task_id)
        if not doc_info or not doc_info[0]:
            info = f"no_doc_info_found_for_task_id ,{task_id}"
            raise RuntimeError(info)
        output_file_path = doc_info[0]['output_file_path']
        output_md_file = save_content_to_md_file(review_result, output_file_path, output_abs_path=True)
        if FileType.XLSX.value ==  criteria_file_type:
            output_file = convert_md_to_xlsx(output_md_file, True)
        else:
            output_file = convert_md_to_docx(output_md_file, True)
        logger.info(f"{uid}, {task_id}, è¯„å®¡æŠ¥å‘Šç”ŸæˆæˆåŠŸ, {output_file}")
        update_process_info(uid, task_id, "è¯„å®¡æŠ¥å‘Šç”Ÿæˆå®Œæ¯•", 100)

    except Exception as e:
        update_process_info(uid, task_id, f"ä»»åŠ¡å¤„ç†å¤±è´¥: {str(e)}")
        logger.exception("è¯„å®¡æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸", e)

def get_reference_from_vdb(keywords: str, vdb_dir: str, llm_cfg: dict) -> str:
    """
    è·å–vdbä¸­ä¸å…³é”®è¯ç›¸å…³çš„æ–‡æœ¬
    :param keywords: å…³é”®è¯
    :param vdb_dir: å‘é‡æ•°æ®åº“ç›®å½•
    :param llm_cfg: ç³»ç»Ÿé…ç½®ä¸­çš„LLM API config
    :return: æ–‡æœ¬
    """
    logger.debug(f"vdb_dir, {vdb_dir}")
    reference = ""
    if not vdb_dir:
        return reference

    try:
        if "" != vdb_dir and os.path.exists(vdb_dir):
            reference = search_txt(keywords, vdb_dir, 0.5, llm_cfg, 2).strip()
        else:
            logger.warning(f"vdb_dir_not_exist: {vdb_dir}, get no references")
            reference = ""
        # logging.info(f"vdb_get_txt:\n{reference}\nby_search_{keywords}")
    except Exception as exp:
        logger.exception(f"get_references_from_vdb_failed, {keywords}")
    return reference





if __name__ == '__main__':
    my_cfg = init_yml_cfg()
    logger.info("my_cfg", my_cfg)
    my_criteria_xlsx_file = "/home/rd/Downloads/1.xlsx"
    my_paper_docx_file = "/home/rd/Downloads/3.docx"
    my_paper_file = convert_docx_to_md(my_paper_docx_file, True)
    logger.info(f"my_paper_file {my_paper_file}")
    my_criteria_file = convert_xlsx_to_md(my_criteria_xlsx_file, True, True)
    logger.info(f"my_criteria_file {my_criteria_file}")
    my_criteria_data = get_md_file_content(my_criteria_file)
    split_md = split_md_file_with_catalogue(my_criteria_file)
    my_review_topic = "å¤©ç„¶æ°”é›¶å”®ä¿¡æ¯ç³»ç»Ÿæ¦‚è¦è®¾è®¡æ–‡æ¡£è¯„å®¡"
    start_ai_review(1, 1, my_review_topic, my_criteria_data, my_paper_file, 0, my_cfg)