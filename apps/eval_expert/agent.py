#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) [2025] [liuyngchng@hotmail.com] - All rights reserved.
import asyncio
import json
import logging.config
import os
import requests
from typing import List, Dict, Any

from pydantic import SecretStr

from common import cfg_util
from common.docx_md_util import convert_docx_to_md
from common.mcp_util import async_get_available_tools
from common.xlsx_md_util import convert_xlsx_to_md

logging.config.fileConfig('logging.conf', encoding="utf-8")
logger = logging.getLogger(__name__)

UPLOAD_FOLDER = 'upload_doc'


class EvalExpertAgent:

    def __init__(self, syc_cfg: dict, prompt_padding=""):
        self.syc_cfg = syc_cfg
        self.llm_api_uri = syc_cfg['api']['llm_api_uri']
        self.llm_api_key = SecretStr(syc_cfg['api']['llm_api_key'])
        self.llm_model_name = syc_cfg['api']['llm_model_name']
        self.mcp_servers = syc_cfg.get('mcp', {}).get('servers', [])
        self.tools = []
        # è¯„å®¡æ ‡å‡†å…³é”®è¯
        self.standard_keywords = [
            'æ ‡å‡†', 'è¦æ±‚', 'å‡†åˆ™', 'è§„èŒƒ', 'æŒ‡å¼•', 'æŒ‡æ ‡',
            'criteria', 'standard', 'requirement', 'guideline',
            'specification', 'rubric', 'evaluation'
        ]

        # é¡¹ç›®ææ–™å…³é”®è¯
        self.material_keywords = [
            'æ–¹æ¡ˆ', 'æŠ¥å‘Š', 'ææ–™', 'æ–‡æ¡£', 'è®¡åˆ’', 'ææ¡ˆ', 'ç”³è¯·', 'è®¾è®¡',
            'proposal', 'report', 'material', 'document', 'submission',
            'application', 'plan', 'project'
        ]

        # æ’é™¤æ–‡ä»¶å…³é”®è¯ï¼ˆå¦‚æ¨¡æ¿ã€æ ·ä¾‹ç­‰ï¼‰
        self.exclude_keywords = [
            'æ¨¡æ¿', 'æ ·ä¾‹', 'ç¤ºä¾‹', 'sample', 'template', 'example'
        ]

    async def initialize_tools(self):
        """å¼‚æ­¥åˆå§‹åŒ–å·¥å…·"""
        try:
            # åŠ¨æ€è·å–å¯ç”¨å·¥å…·
            available_tools = await async_get_available_tools(self.mcp_servers)
            logger.info(f"å¯ç”¨å·¥å…·: {available_tools}")
            self.tools = self.convert_to_basic_tools(available_tools)
            logger.info(f"å·¥å…·åˆå§‹åŒ–å®Œæˆ: {len(self.tools)} ä¸ªå·¥å…·å¯ç”¨")
        except Exception as e:
            logger.error(f"å·¥å…·åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.tools = []

    @staticmethod
    def convert_to_basic_tools(available_tools: List[Dict]) -> List[Dict]:
        """å°†MCPå·¥å…·è½¬æ¢ä¸ºåŸºæœ¬å·¥å…·å­—å…¸"""
        tools = []
        try:
            for tool_info in available_tools:
                tool_dict = {
                    'name': tool_info['name'],
                    'description': tool_info.get('description', ''),
                    'server': tool_info.get('server'),
                    'inputSchema': tool_info.get('inputSchema', {})
                }
                tools.append(tool_dict)

        except Exception as e:
            logger.error(f"è½¬æ¢å·¥å…·å¤±è´¥: {str(e)}")
        return tools

    def call_llm_api(self, prompt: str, stream_callback=None) -> str:
        """ç›´æ¥è°ƒç”¨LLM APIï¼Œæ”¯æŒæµå¼è¾“å‡º"""
        key = self.llm_api_key.get_secret_value()
        model = self.llm_model_name
        uri = f"{self.llm_api_uri}/chat/completions"
        try:
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {key}'
            }

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "user", "content": prompt}
            ]

            # æ„å»ºè¯·æ±‚ä½“
            payload = {
                'model': model,
                'messages': messages,
                'temperature': 1.3,
                'stream': stream_callback is not None  # å¯ç”¨æµå¼è¾“å‡º
            }

            # å¦‚æœæœ‰å¯ç”¨å·¥å…·ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if self.tools:
                payload['tools'] = self.convert_tools_to_openai_format()
                # å¯é€‰ï¼šè®¾ç½®å·¥å…·è°ƒç”¨ç­–ç•¥
                payload['tool_choice'] = 'auto'

            logger.info(f"start_request, {uri}, {model}, å·¥å…·æ•°é‡: {len(self.tools)}, æç¤ºè¯: {prompt[:400]}")

            if stream_callback:
                # æµå¼å¤„ç†
                response = requests.post(
                    url=uri,
                    headers=headers,
                    json=payload,
                    timeout=60,
                    verify=False,
                    stream=True
                )

                if response.status_code == 200:
                    full_content = ""
                    for line in response.iter_lines():
                        if line:
                            line = line.decode('utf-8')
                            if line.startswith('data: '):
                                data = line[6:]
                                if data == '[DONE]':
                                    break
                                try:
                                    chunk = json.loads(data)
                                    if 'choices' in chunk and len(chunk['choices']) > 0:
                                        delta = chunk['choices'][0].get('delta', {})
                                        if 'content' in delta and delta['content']:
                                            content_chunk = delta['content']
                                            full_content += content_chunk
                                            # è°ƒç”¨æµå¼å›è°ƒ
                                            if stream_callback:
                                                stream_callback(content_chunk)
                                except json.JSONDecodeError:
                                    continue
                    return full_content
                else:
                    error_msg = f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return error_msg
            else:
                # éæµå¼å¤„ç†
                response = requests.post(
                    url=uri,
                    headers=headers,
                    json=payload,
                    timeout=60,
                    verify=False,
                )
                logger.info(f"response_status: {response.status_code}")
                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"LLM å“åº”è§£ææˆåŠŸ")
                    return self.parse_llm_response(result)
                else:
                    error_msg = f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return error_msg
        except Exception as e:
            error_msg = f"LLM APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            return error_msg

    def convert_tools_to_openai_format(self) -> List[Dict]:
        """å°†å·¥å…·è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        openai_tools = []
        for tool in self.tools:
            openai_tool = {
                "type": "function",
                "function": {
                    "name": tool['name'],
                    "description": tool.get('description', ''),
                    "parameters": tool.get('inputSchema', {})
                }
            }
            openai_tools.append(openai_tool)
        return openai_tools

    @staticmethod
    def parse_llm_response(result: Dict) -> str:
        """è§£æLLMå“åº”"""
        if 'choices' in result and len(result['choices']) > 0:
            choice = result['choices'][0]
            message = choice.get('message', {})

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if 'tool_calls' in message and message['tool_calls']:
                # è¿”å›å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œè®©ä¸Šå±‚å¤„ç†
                return json.dumps({
                    'tool_calls': message['tool_calls'],
                    'content': message.get('content', '')
                })
            else:
                # ç›´æ¥è¿”å›æ–‡æœ¬å†…å®¹
                return message.get('content', '')
        else:
            return str(result)

    def build_prompt(self, input_data: Dict[str, Any]) -> str:
        """æ„å»ºæç¤ºè¯"""
        template_name = 'eval_expert'
        template = cfg_util.get_usr_prompt_template(template_name, self.syc_cfg)
        if not template:
            raise ReferenceError(f"no_prompt_template_config_for {template_name}")
        # æ›¿æ¢å…¶ä»–å˜é‡
        prompt = template.replace("{today}", str(input_data.get("today", "")))
        prompt = prompt.replace("{domain}", str(input_data.get("domain", "")))
        prompt = prompt.replace("{review_criteria}", str(input_data.get("review_criteria", "")))
        project_material_file = input_data.get("project_material_file", [])
        if isinstance(project_material_file, list):
            project_files_str = ", ".join([
                str(item.get('file_path', '')) if isinstance(item, dict) else str(item)
                for item in project_material_file
            ])
        else:
            project_files_str = str(project_material_file)
        prompt = prompt.replace("{project_material_file}", project_files_str)
        prompt = prompt.replace("{msg}", str(input_data.get("msg", "")))

        return prompt

    async def process_with_tools_stream(self, input_data: Dict[str, Any], stream_callback) -> str:
        """ä½¿ç”¨å·¥å…·å¤„ç†è¯·æ±‚ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
        try:
            if not self.tools:
                await self.initialize_tools()

            # è®°å½•å·¥å…·ä¿¡æ¯
            logger.info(f"å¯ç”¨å·¥å…·æ•°é‡: {len(self.tools)}")
            for tool in self.tools:
                logger.info(f"å·¥å…·: {tool['name']} - {tool['description']}")

            # æ„å»ºæç¤ºè¯
            prompt = self.build_prompt(input_data)

            # å‘é€å¼€å§‹å¤„ç†æ¶ˆæ¯
            stream_callback("ğŸš€ å¼€å§‹å¤„ç†æ‚¨çš„è¯·æ±‚...\n\n")

            # è°ƒç”¨LLM APIï¼ˆæµå¼ï¼‰
            response = self.call_llm_api(prompt, stream_callback)

            # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
            tool_calls = self.extract_tool_calls_from_response(response)

            if tool_calls:
                stream_callback(f"\nğŸ”§ æ£€æµ‹åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨ï¼Œå¼€å§‹æ‰§è¡Œ...\n\n")
                logger.info(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {len(tool_calls)} ä¸ª")
                return await self.handle_tool_calls_stream(tool_calls, input_data, stream_callback)
            else:
                logger.info("æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å“åº”å†…å®¹")
                return response

        except Exception as e:
            error_msg = f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
            logger.error(f"å·¥å…·å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            stream_callback(error_msg)
            return error_msg

    async def handle_tool_calls_stream(self, tool_calls: List, input_data: Dict[str, Any], stream_callback) -> str:
        """å¤„ç†å·¥å…·è°ƒç”¨ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
        try:
            logger.info(f"tool_calls:{tool_calls}")
            tool_results = []
            available_tool_names = [tool['name'] for tool in self.tools]
            logger.info(f"å½“å‰å¯ç”¨å·¥å…·: {available_tool_names}")

            for i, tool_call in enumerate(tool_calls):
                # å‘é€å·¥å…·æ‰§è¡Œè¿›åº¦
                progress_msg = f"ğŸ“‹ æ‰§è¡Œå·¥å…· {i + 1}/{len(tool_calls)}..."
                stream_callback(progress_msg)

                # å¤„ç†ä¸åŒçš„å·¥å…·è°ƒç”¨æ ¼å¼
                if isinstance(tool_call, dict):
                    # å¤„ç† OpenAI æ ¼å¼çš„å·¥å…·è°ƒç”¨
                    if 'function' in tool_call:
                        tool_name = tool_call['function']['name']
                        try:
                            if isinstance(tool_call['function']['arguments'], str):
                                tool_args = json.loads(tool_call['function']['arguments'])
                            else:
                                tool_args = tool_call['function']['arguments']
                        except (json.JSONDecodeError, KeyError) as e:
                            logger.error(f"å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}")
                            tool_args = {}
                    elif 'name' in tool_call:
                        tool_name = tool_call['name']
                        tool_args = tool_call.get('args', {})
                    elif 'tool_name' in tool_call:
                        tool_name = tool_call['tool_name']
                        tool_args = tool_call.get('parameters', {})
                    else:
                        continue
                else:
                    continue

                # å…³é”®ä¿®å¤ï¼šå¤„ç† __arg1 å‚æ•°æ ¼å¼
                if isinstance(tool_args, dict) and '__arg1' in tool_args:
                    # å°† {'__arg1': 'file_path'} è½¬æ¢ä¸º {'file_path': 'file_path'}
                    file_path = tool_args['__arg1']
                    tool_args = {'file_path': file_path}
                    logger.info(f"è½¬æ¢å‚æ•°æ ¼å¼: {tool_args}")

                # å‘é€å·¥å…·æ‰§è¡Œå¼€å§‹æ¶ˆæ¯
                start_msg = f"\nğŸ› ï¸ æ­£åœ¨æ‰§è¡Œå·¥å…·: **{tool_name}**\n"
                stream_callback(start_msg)
                logger.info(f"æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {tool_args}")

                tool = next((t for t in self.tools if t['name'] == tool_name), None)

                if tool:
                    try:
                        # æ‰§è¡Œå·¥å…·è°ƒç”¨
                        result = self.execute_tool(tool, tool_args)
                        logger.info(f"å·¥å…· {tool_name} è°ƒç”¨æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(str(result))}")

                        # å‘é€å·¥å…·æ‰§è¡ŒæˆåŠŸæ¶ˆæ¯
                        success_msg = f"âœ… å·¥å…· **{tool_name}** æ‰§è¡ŒæˆåŠŸï¼\n"
                        if len(str(result)) > 200:
                            success_msg += f"ğŸ“Š è¿”å›ç»“æœé•¿åº¦: {len(str(result))} å­—ç¬¦\n"
                        else:
                            success_msg += f"ğŸ“Š è¿”å›ç»“æœ: {str(result)[:100]}...\n"
                        stream_callback(success_msg)

                        tool_results.append({
                            'tool': tool_name,
                            'result': result
                        })
                    except Exception as e:
                        logger.error(f"å·¥å…· {tool_name} è°ƒç”¨å¤±è´¥: {str(e)}")
                        error_msg = f"âŒ å·¥å…· **{tool_name}** æ‰§è¡Œå¤±è´¥: {str(e)}\n"
                        stream_callback(error_msg)
                        tool_results.append({
                            'tool': tool_name,
                            'result': f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"
                        })
                else:
                    logger.warning(f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}")
                    warning_msg = f"âš ï¸ æœªæ‰¾åˆ°å·¥å…·: {tool_name}\n"
                    stream_callback(warning_msg)
                    tool_results.append({
                        'tool': tool_name,
                        'result': f"å·¥å…·æœªæ‰¾åˆ°: {tool_name}"
                    })

            if tool_results:
                stream_callback("\nğŸ¯ å·¥å…·è°ƒç”¨å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆå“åº”...\n\n")
                logger.info(f"å·¥å…·è°ƒç”¨å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæœ€ç»ˆå“åº”")
                return await self.finalize_with_tool_results_stream(input_data, tool_results, stream_callback)
            else:
                no_tools_msg = "æœªæ‰§è¡Œä»»ä½•å·¥å…·è°ƒç”¨"
                stream_callback(no_tools_msg)
                return no_tools_msg

        except Exception as e:
            error_msg = f"âŒ å¤„ç†å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {str(e)}"
            logger.error(f"å¤„ç†å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {str(e)}")
            stream_callback(error_msg)
            return error_msg

    async def real_call_llm_api_stream(self, prompt: str, stream_callback) -> str:
        """çœŸæ­£çš„æµå¼LLM APIè°ƒç”¨"""
        key = self.llm_api_key.get_secret_value()
        model = self.llm_model_name
        uri = f"{self.llm_api_uri}/chat/completions"

        try:
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {key}'
            }

            messages = [{"role": "user", "content": prompt}]

            payload = {
                'model': model,
                'messages': messages,
                'temperature': 0.7,
                'stream': True
            }

            if self.tools:
                payload['tools'] = self.convert_tools_to_openai_format()
                payload['tool_choice'] = 'auto'

            logger.info(f"æµå¼è¯·æ±‚å¼€å§‹, {uri}, {model}")

            # ä½¿ç”¨aiohttpæ›¿ä»£requestsä»¥è·å¾—æ›´å¥½çš„å¼‚æ­¥æ”¯æŒ
            import aiohttp
            full_content = ""

            async with aiohttp.ClientSession() as session:
                async with session.post(uri, headers=headers, json=payload, ssl=False) as response:
                    if response.status == 200:
                        async for line in response.content:
                            if line:
                                line = line.decode('utf-8').strip()
                                if line.startswith('data: '):
                                    data = line[6:]
                                    if data == '[DONE]':
                                        break
                                    try:
                                        chunk = json.loads(data)
                                        if 'choices' in chunk and chunk['choices']:
                                            delta = chunk['choices'][0].get('delta', {})
                                            if 'content' in delta and delta['content']:
                                                content_chunk = delta['content']
                                                full_content += content_chunk
                                                # è°ƒç”¨æµå¼å›è°ƒ
                                                stream_callback(content_chunk)
                                    except json.JSONDecodeError:
                                        continue
                        return full_content
                    else:
                        error_text = await response.text()
                        error_msg = f"LLM APIè°ƒç”¨å¤±è´¥: {response.status}"
                        logger.error(f"{error_msg} - {error_text}")
                        stream_callback(error_msg)
                        return error_msg

        except Exception as e:
            error_msg = f"LLM APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            stream_callback(error_msg)
            return error_msg

    async def real_handle_tool_calls_stream(self, tool_calls: List, input_data: Dict[str, Any], stream_callback) -> str:
        """çœŸæ­£çš„æµå¼å·¥å…·è°ƒç”¨å¤„ç†"""
        try:
            tool_results = []

            for i, tool_call in enumerate(tool_calls):
                # å‘é€è¿›åº¦
                stream_callback(f"ğŸ“‹ æ‰§è¡Œå·¥å…· {i + 1}/{len(tool_calls)}...")

                # å·¥å…·è°ƒç”¨é€»è¾‘ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰
                # ... çœç•¥å…·ä½“å·¥å…·æ‰§è¡Œé€»è¾‘ ...

                # æ‰§è¡Œå·¥å…·
                tool_name = tool_call['function']['name']
                stream_callback(f"\nğŸ› ï¸ æ­£åœ¨æ‰§è¡Œå·¥å…·: **{tool_name}**\n")

                # æ‰§è¡Œå·¥å…·å¹¶å‘é€ç»“æœ
                result = self.execute_tool(tool, tool_args)
                stream_callback(f"âœ… å·¥å…· **{tool_name}** æ‰§è¡ŒæˆåŠŸï¼\n")

                tool_results.append({
                    'tool': tool_name,
                    'result': result
                })

            # ç”Ÿæˆæœ€ç»ˆå“åº”
            stream_callback("\nğŸ¯ å·¥å…·è°ƒç”¨å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆå“åº”...\n\n")

            final_prompt = self.build_final_prompt(input_data, tool_results)
            final_response = await self.real_call_llm_api_stream(final_prompt, stream_callback)

            return final_response

        except Exception as e:
            error_msg = f"âŒ å¤„ç†å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {str(e)}"
            logger.error(f"å¤„ç†å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {str(e)}")
            stream_callback(error_msg)
            return error_msg

    async def finalize_with_tool_results_stream(self, input_data: Dict[str, Any], tool_results: List[Dict],
                                                stream_callback) -> str:
        """ä½¿ç”¨å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå“åº”ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
        try:
            # æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æç¤ºè¯
            tool_results_str = "\n".join([
                f"å·¥å…· {result['tool']} ç»“æœ: {result['result'][:200]}..." if len(
                    str(result['result'])) > 200 else f"å·¥å…· {result['tool']} ç»“æœ: {result['result']}"
                for result in tool_results
            ])

            final_prompt = f"""
            åŸºäºåŸå§‹è¯·æ±‚å’Œå·¥å…·è°ƒç”¨ç»“æœï¼Œè¯·ç»™å‡ºæœ€ç»ˆçš„è¯„å®¡ç»“è®ºã€‚

            åŸå§‹è¯·æ±‚:
            - é¢†åŸŸ: {input_data.get('domain', '')}
            - è¯„å®¡ä¾æ®ä¸æ ‡å‡†: {input_data.get('review_criteria', '')}
            - é¡¹ç›®ææ–™æ–‡ä»¶å: {input_data.get('project_material_file', '')}
            - ç”¨æˆ·æ¶ˆæ¯: {input_data.get('msg', '')}

            å·¥å…·è°ƒç”¨ç»“æœ:
            {tool_results_str}

            è¯·ä¸¥æ ¼æŒ‰ç…§è¯„å®¡ä¾æ®ä¸æ ‡å‡†æ¨¡æ¿ï¼š
            {input_data.get('review_criteria', '')}

            å¡«å†™æ¨¡æ¿ä¸­çš„ç›¸åº”å†…å®¹ï¼Œç»™å‡ºæœ€ç»ˆçš„æŠ¥å‘Šã€‚
            """

            stream_callback("ğŸ“ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...\n\n")
            return self.call_llm_api(final_prompt, stream_callback)

        except Exception as e:
            logger.error(f"ç”Ÿæˆæœ€ç»ˆå“åº”æ—¶å‡ºé”™: {str(e)}")
            error_msg = "âŒ å¤„ç†å®Œæˆï¼Œä½†ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šæ—¶å‡ºç°é”™è¯¯ã€‚"
            stream_callback(error_msg)
            return error_msg



    @staticmethod
    def execute_tool(tool: Dict, tool_args: Dict) -> str:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        try:
            server_addr = tool.get('server')
            if not server_addr:
                return f"å·¥å…· {tool['name']} æœªé…ç½®æœåŠ¡å™¨åœ°å€"

            headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json, text/event-stream'
            }

            # æ„å»ºç¬¦åˆ JSON-RPC 2.0 æ ‡å‡†çš„è¯·æ±‚ä½“
            import uuid
            json_rpc_request = {
                "jsonrpc": "2.0",
                "id": str(uuid.uuid4()),
                "method": "tools/call",
                "params": {
                    "name": tool['name'],
                    "arguments": tool_args
                }
            }

            logger.info(f"è°ƒç”¨å·¥å…· {server_addr}, {tool['name']}, å‚æ•°: {tool_args}")
            response = requests.post(
                f"{server_addr}",
                headers=headers,
                json=json_rpc_request,  # ä½¿ç”¨æ ‡å‡†çš„ JSON-RPC 2.0 æ ¼å¼
                timeout=30,
                verify=False
            )

            if response.status_code == 200:
                result_data = response.json()
                # æ£€æŸ¥ JSON-RPC å“åº”æ ¼å¼
                if 'result' in result_data:
                    result = result_data['result']
                    logger.info(f"å·¥å…· {tool['name']} è°ƒç”¨æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(str(result))}")
                    return result
                elif 'error' in result_data:
                    error_msg = f"å·¥å…·è°ƒç”¨è¿”å›é”™è¯¯: {result_data['error']}"
                    logger.error(f"å·¥å…· {tool['name']} è°ƒç”¨é”™è¯¯: {error_msg}")
                    return error_msg
                else:
                    error_msg = f"æ— æ•ˆçš„ JSON-RPC å“åº”æ ¼å¼: {result_data}"
                    logger.error(f"å·¥å…· {tool['name']} å“åº”æ ¼å¼é”™è¯¯: {error_msg}")
                    return error_msg
            else:
                error_msg = f"å·¥å…·è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
                logger.error(f"å·¥å…· {tool['name']} è°ƒç”¨å¤±è´¥: {error_msg}")
                return error_msg

        except Exception as e:
            error_msg = f"å·¥å…·è°ƒç”¨å¼‚å¸¸: {str(e)}"
            logger.error(f"å·¥å…· {tool['name']} è°ƒç”¨å¼‚å¸¸: {error_msg}")
            return error_msg

    def extract_tool_calls_from_response(self, response: str) -> List[Dict]:
        """ä»LLMå“åº”ä¸­æå–å·¥å…·è°ƒç”¨ä¿¡æ¯"""
        tool_calls = []

        try:
            # é¦–å…ˆå°è¯•è§£æä¸ºJSON
            if response.startswith('{') and response.endswith('}'):
                try:
                    data = json.loads(response)
                    # æ£€æŸ¥OpenAIæ ¼å¼çš„å·¥å…·è°ƒç”¨
                    if 'tool_calls' in data and data['tool_calls']:
                        return data['tool_calls']
                    # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„å·¥å…·è°ƒç”¨æ ¼å¼
                    if 'function_calls' in data and data['function_calls']:
                        return data['function_calls']
                except json.JSONDecodeError:
                    pass

            # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–å·¥å…·è°ƒç”¨ï¼Œæ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«å·¥å…·åç§°
            if self.tools:
                for tool in self.tools:
                    tool_name = tool['name']
                    # æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦åœ¨å“åº”ä¸­æ˜ç¡®æåˆ°
                    if (tool_name.lower() in response.lower() or
                            f'"{tool_name}"' in response or
                            f"'{tool_name}'" in response):
                        # å°è¯•æå–å‚æ•°
                        tool_args = self.extract_tool_arguments(response, tool_name)

                        # åˆ›å»ºå·¥å…·è°ƒç”¨å¯¹è±¡
                        tool_call = {
                            'id': f"call_{len(tool_calls) + 1}",
                            'type': 'function',
                            'function': {
                                'name': tool_name,
                                'arguments': json.dumps(tool_args) if tool_args else '{}'
                            }
                        }
                        tool_calls.append(tool_call)
                        logger.info(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool_name} with args: {tool_args}")

        except Exception as e:
            logger.error(f"æå–å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}")

        logger.info(f"æœ€ç»ˆæå–çš„å·¥å…·è°ƒç”¨: {len(tool_calls)} ä¸ª")
        return tool_calls

    @staticmethod
    def extract_tool_arguments(response: str, tool_name: str) -> Dict:
        """ä»å“åº”ä¸­æå–å·¥å…·å‚æ•°"""
        # ç®€å•çš„å‚æ•°æå–é€»è¾‘ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚å¢å¼º
        args = {}

        # æŸ¥æ‰¾æ–‡ä»¶è·¯å¾„å‚æ•°
        if 'file_path' in response.lower() or 'æ–‡ä»¶' in response:
            # å°è¯•æå–æ–‡ä»¶è·¯å¾„
            import re
            file_patterns = [
                r'file_path[\"\' ]*:[\"\' ]*([^\"\',}\s]+)',
                r'æ–‡ä»¶[\"\' ]*:[\"\' ]*([^\"\',}\s]+)',
                r'path[\"\' ]*:[\"\' ]*([^\"\',}\s]+)'
            ]

            for pattern in file_patterns:
                matches = re.findall(pattern, response, re.IGNORECASE)
                if matches:
                    args['file_path'] = matches[0]
                    break

        return args

    @staticmethod
    def get_file_path_msg(categorize_files: dict[str, list[str]], content_type: str) -> list[dict]:
        msg = []
        for file_name in categorize_files.get(content_type, []):
            logger.info(f"processing_file: {file_name}")
            file_path = EvalExpertAgent.get_file_convert_md_file_path(file_name)
            msg.append({
                'file_path': file_path
            })
        return msg

    @staticmethod
    def get_file_convert_md_file_path(file_name: str) -> str:
        """
        æ ¹æ®æ–‡ä»¶ä¿¡æ¯è·å–æ–‡ä»¶è½¬æ¢ä¸ºmarkdown æ–‡ä»¶çš„ç£ç›˜è·¯å¾„
        """
        file_path = os.path.join(UPLOAD_FOLDER, file_name)
        abs_path = os.path.abspath(file_path)
        if not os.path.exists(abs_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {abs_path}")
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©å¤„ç†æ–¹æ³•
        file_ext = os.path.splitext(file_name)[1].lower()

        try:
            if file_ext in ['.docx', '.doc']:
                return convert_docx_to_md(file_path)
            elif file_ext in ['.xlsx', '.xls']:
                return convert_xlsx_to_md(file_path)
            elif file_ext == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return file_path
            elif file_ext == '.pdf':
                # å¦‚æœéœ€è¦å¤„ç†PDFï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ PDFå¤„ç†é€»è¾‘
                return f"PDFæ–‡ä»¶å†…å®¹æå–åŠŸèƒ½å¾…å®ç°: {file_name}"
            else:
                return f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"
        except Exception as e:
            logger.error(f"file_processing_error: {file_name}, {str(e)}")
            return f"æ–‡ä»¶å¤„ç†å¤±è´¥ {file_name}: {str(e)}"

    def categorize_files(self, file_infos: list[dict[str, str]]) -> dict[str, list[str]]:
        """
        æ ¹æ®æ–‡ä»¶åå¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç±»

        Args:
            file_infos: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ [{"file_id":"my_file_id", "file_name":"my_file_name"}]

        Returns:
            åˆ†ç±»åçš„å­—å…¸ï¼Œ {"review_criteria":["file_name1", "file_name2"], "project_materials":["file_name3", "file_name4"]}
        """
        standards = []
        materials = []
        uncategorized = []

        for file_item in file_infos:
            file_name = file_item.get('file_name', '').lower()
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ’é™¤æ–‡ä»¶
            if any(keyword in file_name for keyword in self.exclude_keywords):
                logger.info(f"æ–‡ä»¶ {file_name} è¢«æ’é™¤ï¼ˆæ¨¡æ¿/æ ·ä¾‹æ–‡ä»¶ï¼‰")
                continue
            if any(keyword in file_name for keyword in self.standard_keywords):
                standards.append(file_item.get('file_name', ''))
                logger.info(f"æ–‡ä»¶ {file_name} åˆ†ç±»ä¸º: è¯„å®¡æ ‡å‡†")
            elif any(keyword in file_name for keyword in self.material_keywords):
                materials.append(file_item.get('file_name', ''))
                logger.info(f"æ–‡ä»¶ {file_name} åˆ†ç±»ä¸º: é¡¹ç›®ææ–™")
            else:
                uncategorized.append(file_item.get('file_name', ''))
        # è®°å½•åˆ†ç±»ç»“æœ
        logger.info(f"åˆ†ç±»å®Œæˆ: è¯„å®¡æ ‡å‡† {len(standards)} ä¸ª, "
                    f"é¡¹ç›®ææ–™ {len(materials)} ä¸ª, æœªåˆ†ç±» {len(uncategorized)} ä¸ª")

        return {
            'review_criteria': standards,
            'project_materials': materials,
            'uncategorized': uncategorized
        }